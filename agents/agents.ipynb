{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23de6dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6088 sha256=d0b6844d1c8d47f3d380aed0c9b9859ea9ef135e24c423e43569094b2928c41d\n",
      "  Stored in directory: /home/navas/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53d1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5029f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wikiTool=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa138191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiTool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400361b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is machine learning',\n",
       " 'result': 'Machine learning (ML) is a field of artificial intelligence that focuses on developing statistical algorithms capable of learning from data and applying that knowledge to make predictions or classifications on new, unseen data. \\n'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq # type: ignore\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "### LLM based webloader retrievel\n",
    "\n",
    "\n",
    "loader=WebBaseLoader(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
    "docs=loader.load()\n",
    "\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "vectordb=FAISS.from_documents(documents, HuggingFaceEmbeddings())\n",
    "\n",
    "llm = ChatGroq(model_name=\"gemma2-9b-it\",temperature=0.7)\n",
    "\n",
    "\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever\n",
    "\n",
    "\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "\n",
    "result=qa_chain_mr.invoke({\"query\": 'What is machine learning'})\n",
    "result\n",
    "\n",
    "# retriever.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# retriever_tool=create_retriever_tool(retriever, 'machine_learning', 'Search for information about machine learing, For any question about machine learing please use this tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e81adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine_learning'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b84c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Arxiv tool\n",
    "\n",
    "# from langchain_community.utilities import ArxivAPIWrapper\n",
    "# from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "# arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "# arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "# arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ab7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/navas/Documents/Projects/LLM/langchain_rag/venv/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='machine_learning', description='Search for information about machine learing, For any question about machine learing please use this tool', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7aff66269580>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7afe765a1160>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7aff65f61da0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7afe765a1160>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tools=[wikiTool, arxiv, retriever_tool]\n",
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from langchain import hub\n",
    "# # Get the prompt to use - you can modify this!\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# prompt.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Agents\n",
    "# from langchain.agents import create_openai_tools_agent\n",
    "# agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "\n",
    "\n",
    "# # Agent Executer\n",
    "# from langchain.agents import AgentExecutor\n",
    "# agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "# agent_executor\n",
    "\n",
    "# agent_executor.invoke({\"input\":\"Tell me about Machine Learing\"})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
